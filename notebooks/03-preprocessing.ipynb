{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_utils import showAttribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: eliana.preprocessing\n",
    "\n",
    "Classes to preprocess the events, similar to NLP preprocessing but oriented specifically to event logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "The tokenizers in eliana.preprocessing module support multiple inheritance to implement complex behavior based on regular expressions. New tokenizers can be created from the base tokenizers shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Tokenizers\n",
    "\n",
    "The AbstractTokenizer provides the interface for the rest of tokenizers used in eliana.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractTokenizer   : Main methods for tokenizers.\n",
      "\n",
      "Constructor\n",
      "================\n",
      "__init__            : Initializes the AbstractTokenizer with optional keyword arguments.\n",
      "\n",
      "Properties\n",
      "================\n",
      "options             : (bool) remove_extra_spaces, to_lowercase, strip\n",
      "\n",
      "Methods\n",
      "================\n",
      "help                : Provides help information about the tokenizer class, including its inheritance and tokenization order.\n",
      "normalize           : Applies option normalization to the input text.\n",
      "tokenize            : Apply tokenization to the provided text.\n"
     ]
    }
   ],
   "source": [
    "from eliana.preprocessing import AbstractTokenizer\n",
    "showAttribs(AbstractTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eliana.preprocessing import RegExpTokenizer, Numbers, UTCdate, Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegExpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class \"RegExpTokenizer\"\n",
      "=================================\n",
      "Inherits from:\n",
      "Options: namespace(remove_extra_spaces=False, to_lowercase=False, strip=False)\n",
      "\n",
      "Tokenization is done in the following order\n",
      "\n",
      "RegExpTokenizer:     A tokenizer that uses regular expressions to tokenize text. Inherits from AbstractTokenizer.\n"
     ]
    }
   ],
   "source": [
    "print(RegExpTokenizer().help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'remove_extra_spaces': True, 'to_lowercase': False, 'strip': False}\n",
      "Regexps: [('(\\\\{\\\\})+', '{}'), ('xx(.+)xx', '\\\\1')]\n",
      "Original : 'xxabcxx      jj cc'\n",
      "Tokenized: 'abc jj cc'\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tkn = RegExpTokenizer(remove_extra_spaces=True)\n",
    "tkn.add_regexp( r\"xx(.+)xx\", r\"\\1\")\n",
    "\n",
    "text = \"xxabcxx      jj cc\"\n",
    "\n",
    "print(f\"Options: {tkn.options.__dict__}\")\n",
    "print(f\"Regexps: {tkn.regexps}\")\n",
    "print(f\"Original : '{text}'\")\n",
    "print(f\"Tokenized: '{tkn.tokenize(text)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class \"Numbers\"\n",
      "=========================\n",
      "Inherits from: RegExpTokenizer\n",
      "Options: namespace(remove_extra_spaces=False, to_lowercase=False, strip=False)\n",
      "\n",
      "Tokenization is done in the following order\n",
      "\n",
      "RegExpTokenizer:     A tokenizer that uses regular expressions to tokenize text. Inherits from AbstractTokenizer.\n",
      "\n",
      "Numbers: Transform numbers using {} as token, but ignore numbers that are parts of a word\n",
      "    1 -> {}\n",
      "    -10.54 -> {}\n",
      "    9.1e-2.1 -> {}\n",
      "    There are 2 telescopes: UT1, UT2 -> There are {} telescopes: UT1, UT2\n",
      "    2Good2be_True -> 2Good2be_True\n"
     ]
    }
   ],
   "source": [
    "print(Numbers().help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'remove_extra_spaces': False, 'to_lowercase': False, 'strip': False}\n",
      "Original : 'abc 123 456.78 Not99Tokenized'\n",
      "Tokenized: 'abc {} {} Not99Tokenized'\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tkn = Numbers()\n",
    "tkn.tokenize(\"abc 123 456.78 Not99Tokenized\")\n",
    "text = \"abc 123 456.78 Not99Tokenized\"\n",
    "print(f\"Options: {tkn.options.__dict__}\")\n",
    "print(f\"Original : '{text}'\")\n",
    "print(f\"Tokenized: '{tkn.tokenize(text)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTCdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class \"UTCdate\"\n",
      "=========================\n",
      "Inherits from: RegExpTokenizer\n",
      "Options: namespace(remove_extra_spaces=False, to_lowercase=False, strip=False)\n",
      "\n",
      "Tokenization is done in the following order\n",
      "\n",
      "RegExpTokenizer:     A tokenizer that uses regular expressions to tokenize text. Inherits from AbstractTokenizer.\n",
      "\n",
      "UTCdate: Transform UTC dates using {} as token\n",
      "    2022-10-01T00:43:01.123 -> {}\n",
      "    Started at 2019-04-01T22:29:07 (underlined) -> Started at {} (underlined)\n"
     ]
    }
   ],
   "source": [
    "print(UTCdate().help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc {} {} smtg else'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "tkn = UTCdate() \n",
    "tkn.tokenize(\"abc 2019-12-31T00:00:00 2019-12-31T23:59:59.999 smtg else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class \"Punctuation\"\n",
      "=============================\n",
      "Inherits from: RegExpTokenizer\n",
      "Options: namespace(remove_extra_spaces=False, to_lowercase=False, strip=False)\n",
      "\n",
      "Tokenization is done in the following order\n",
      "\n",
      "RegExpTokenizer:     A tokenizer that uses regular expressions to tokenize text. Inherits from AbstractTokenizer.\n",
      "\n",
      "Punctuation: Remove all punctuation\n",
      "    Original: Hi! I'm counting 1,2, 3 ... and so on.\n",
      "    Tokenized: Hi I m counting 1 2 3 and so on\n"
     ]
    }
   ],
   "source": [
    "print(Punctuation().help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi  I m counting 1 2  3     and so on '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "tkn = Punctuation() \n",
    "tkn.tokenize(\"Hi! I'm counting 1,2, 3 ... and so on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composed Tokenizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VltTokenizer(UTCdate, Numbers, Punctuation):\n",
    "    \"\"\" Composing example\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        kwargs.setdefault('remove_extra_spaces', True)\n",
    "        kwargs.setdefault('to_lowercase', True)\n",
    "        kwargs.setdefault('strip', True)\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class \"VltTokenizer\"\n",
      "==============================\n",
      "Inherits from: UTCdate, Numbers, Punctuation, RegExpTokenizer\n",
      "Options: namespace(remove_extra_spaces=True, to_lowercase=True, strip=True)\n",
      "\n",
      "Tokenization is done in the following order\n",
      "\n",
      "UTCdate: Transform UTC dates using {} as token\n",
      "    2022-10-01T00:43:01.123 -> {}\n",
      "    Started at 2019-04-01T22:29:07 (underlined) -> Started at {} (underlined)\n",
      "\n",
      "Numbers: Transform numbers using {} as token, but ignore numbers that are parts of a word\n",
      "    1 -> {}\n",
      "    -10.54 -> {}\n",
      "    9.1e-2.1 -> {}\n",
      "    There are 2 telescopes: UT1, UT2 -> There are {} telescopes: UT1, UT2\n",
      "    2Good2be_True -> 2Good2be_True\n",
      "\n",
      "Punctuation: Remove all punctuation\n",
      "    Original: Hi! I'm counting 1,2, 3 ... and so on.\n",
      "    Tokenized: Hi I m counting 1 2 3 and so on\n",
      "\n",
      "RegExpTokenizer:     A tokenizer that uses regular expressions to tokenize text. Inherits from AbstractTokenizer.\n",
      "\n",
      "VltTokenizer:  Composing example\n"
     ]
    }
   ],
   "source": [
    "print(VltTokenizer().help())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wat2tcs lt3aga w2fors bobwish_105797 iteration {} bob_234 lt4aag cmd77 {}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn = VltTokenizer()\n",
    "tkn.tokenize(\"wat2tcs lt3aga w2fors (bobWish_105797) ITERATION=10 [bob_234] lt4aag cmd77 2022-10-01T00:43:01.123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wat2tcs lt3aga w2fors bobWish_105797 ITERATION {} bob_234 lt4aag cmd77 {}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn = VltTokenizer(to_lowercase=False)\n",
    "tkn.tokenize(\"wat2tcs lt3aga w2fors (bobWish_105797) ITERATION=10 [bob_234] lt4aag cmd77 2022-10-01T00:43:01.123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Colorizer\n",
    "Learn from statistical properties in a trace of logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogColorizer        : Model to learn tokenizers from a dataset of traces.\n",
      "\n",
      "Constructor\n",
      "================\n",
      "__init__            : Initializes the LogColorizer with a tokenizer or a custom tokenization function.\n",
      "\n",
      "Properties\n",
      "================\n",
      "regexps             : List of regular expressions used by the tokenizer, including post-processing patterns.\n",
      "special             : Symbol used as a placeholder for numeric or variable values in templates. Default is 'ยง'.\n",
      "templates           : List of templates used for matching traces.\n",
      "wildcard            : Placeholder for variable text in templates. Default is \"{}\".\n",
      "\n",
      "Methods\n",
      "================\n",
      "fit                 : Learns templates and tokenization rules from a dataset of traces.\n",
      "fit_on_traces       : None\n",
      "save                : Saves the tokenizer object to a file.\n",
      "tokenize            : Tokenizes a word or phrase using the provided tokenizer and post-processing steps.\n"
     ]
    }
   ],
   "source": [
    "from eliana.preprocessing import LogColorizer\n",
    "showAttribs(LogColorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [LogColorizer](03-preprocessing.log_colorizer.ipynb) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
